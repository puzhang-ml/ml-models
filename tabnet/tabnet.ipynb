{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, Pu tries to implement the TabNet model in Pytorch. This is based on the tutorial here: https://towardsdatascience.com/implementing-tabnet-in-pytorch-fc977c383279 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBN(nn.Module):\n",
    "    def __init__(self, inp, vbs=128, momentum=0.01):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm1d(inp, momentum=momentum)\n",
    "        self.vbs = vbs\n",
    "\n",
    "    def forward(self, x):\n",
    "        chunks = torch.chunk(x, x.size(0)//self.vbs, 0)\n",
    "        res = [res.bn(y) for y in chunks]\n",
    "        return torch.concat(res, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Sparsemax(nn.Module):\n",
    "    def __init__(self, dim=None):\n",
    "        super(Sparsemax, self).__init__()\n",
    "        self.dim = -1 if dim is None else dim\n",
    "\n",
    "    def forward(self, input):\n",
    "        # if no dimension is specified, the operation will be applied to the last dimension of the tensor\n",
    "        if dim is None:\n",
    "            dim = -1\n",
    "\n",
    "        number_of_dims = len(input.size())\n",
    "        dim = range(number_of_dims)[dim]\n",
    "\n",
    "        # tarnspose the input tensor so that the dimension we are interested in becomes the last dim\n",
    "        input_transposed = input.transpose(dim, -1)\n",
    "        original_size = input_transposed.size()\n",
    "\n",
    "        # the transposed tensor is then reshaped into a 2D tensor\n",
    "        input_2d = input_transposed.contiguous().view(-1, input_transposed.size(-1))\n",
    "        \n",
    "        number_of_feats = input_2d.size(1)\n",
    "\n",
    "        # the tensor is sorted in desending order along the second dimension the cumulative sum is calculated and \n",
    "        # -1 is subtracted from the cumsum and this will be used as threshold later\n",
    "        input_sorted, _ = torch.sort(input_2d, descending=True, dim=1)\n",
    "        input_cumsum = input_sorted.cumsum(dim=1) - 1\n",
    "        \n",
    "        # a range of values from 1 to `number_of_feats + 1` is created and then a count of how many elements in each\n",
    "        # vector of `input_sorted` are greater than the corresponding element in `input_cumsum / arrange` is calculated\n",
    "        arange = torch.arange(1, number_of_feats + 1, device=input.device)\n",
    "        counts = (input_sorted > input_cumsum / arange).sum(dim=1).unsqueeze(1)\n",
    "        \n",
    "        # the `gather` function is used to select the elements in each row of `input_cumsum` specified by `counts - 1`,\n",
    "        # which gives the cumsum value at the threshold index for each row, and then that is divided by `counts` to get \n",
    "        # threshold for each vector in the input\n",
    "        threshold = input_cumsum.gather(1, counts - 1) / counts.to(input.dtype)\n",
    "\n",
    "        # the threshold is subtracted from the original 2D tensor, and any negative values are clamped to zero. This\n",
    "        # creates sparsity in the output. The tensor is then reshaped back to the original size and the tranpose is undone\n",
    "        output = torch.clamp(input_2d - threshold, min=0)\n",
    "        output = output.view(*original_size).transpose(dim, -1).contiguous()\n",
    "\n",
    "        self.output = output\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \n",
    "\n",
    "sparsemax(torch.tensor([1, 1, 1, 1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, vbs=128, momentum=0.02):\n",
    "        super(AttentionTransformer, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim, bias=False)\n",
    "        self.bn = GBN(output_dim, vbs=vbs, momentum=momentum)\n",
    "        self.max = Sparsemax()\n",
    "    \n",
    "    def forward(self, priors, processed_feat):\n",
    "        x = self.bn(self.fc(processed_feat))\n",
    "        x = torch.mul(x, priors)\n",
    "        x = self.sparsemax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class FeatureTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, n_shared=2, n_individual=2, vbs=128, momentum=0.02):\n",
    "        super(FeatureTransformer, self).__init__()\n",
    "\n",
    "        # the shared layers are used by all decision steps\n",
    "        self.shared_layers = nn.ModuleList()\n",
    "        for _ in range(n_shared):\n",
    "            self.shared_layers.append(nn.Linear(input_dim, output_dim, bias=False))\n",
    "            self.shared_layers.append(GBN(input_dim, vbs=vbs, momentum=momentum))\n",
    "\n",
    "        self.individual_layers = nn.ModuleList()\n",
    "        for _ in range(n_individual):\n",
    "            self.individual_layers.append(nn.Linear(output_dim, output_dim, bias=False))\n",
    "            self.individual_layers.append(GBN(output_dim, vbs=vbs, momentum=momentum))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.shared_layers:\n",
    "            x = F.relu(layer(x))\n",
    "\n",
    "        for layer in self.individual_layers:\n",
    "            x = F.relu(layer(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, n_steps, vbs=128, momentum=0.02):\n",
    "        super(TabNet, self).__init__()\n",
    "        self.n_steps = n_steps\n",
    "        self.attention_transformers = nn.ModuleList()\n",
    "        self.feature_transformers = nn.ModuleList()\n",
    "        for _ in range(n_steps):\n",
    "            attention_transformer = AttentionTransformer(input_dim, output_dim, vbs=vbs, momentum=momentum)\n",
    "            self.attention_transformers.append(attention_transformer)\n",
    "            feature_transformer = FeatureTransformer(input_dim, output_dim, vbs=vbs, momentum=momentum)\n",
    "            self.feature_transformers.append(feature_transformer)\n",
    "        self.final_mapping = nn.Linear(output_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        prior = torch.ones(x.shape, device=x.device) / x.shape[1]\n",
    "        for step in range(self.n_steps):\n",
    "            attention_score = self.attention_transformers[step](prior, x)\n",
    "            x = self.feature_transformers[step](x)\n",
    "            prior = attention_score\n",
    "        return self.final_mapping(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
