{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, Pu tries to implement the TabNet model in Pytorch. This is based on the tutorial here: https://towardsdatascience.com/implementing-tabnet-in-pytorch-fc977c383279 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBN(nn.Module):\n",
    "    def __init__(self, inp, vbs=128, momentum=0.01):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm1d(inp, momentum=momentum)\n",
    "        self.vbs = vbs\n",
    "\n",
    "    def forward(self, x):\n",
    "        chunks = torch.chunk(x, x.size(0)//self.vbs, 0)\n",
    "        res = [self.bn(y) for y in chunks]\n",
    "        return torch.concat(res, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sparsemax(nn.Module):\n",
    "    def __init__(self, dim=None):\n",
    "        super(Sparsemax, self).__init__()\n",
    "        # if no dimension is specified, the operation will be applied to the last dimension of the tensor\n",
    "        self.dim = -1 if dim is None else dim\n",
    "\n",
    "    def forward(self, input):\n",
    "        number_of_dims = len(input.size())\n",
    "        dim = range(number_of_dims)[self.dim]\n",
    "\n",
    "        # tarnspose the input tensor so that the dimension we are interested in becomes the last dim\n",
    "        input_transposed = input.transpose(dim, -1)\n",
    "        original_size = input_transposed.size()\n",
    "\n",
    "        # the transposed tensor is then reshaped into a 2D tensor\n",
    "        input_2d = input_transposed.contiguous().view(-1, input_transposed.size(-1))\n",
    "        \n",
    "        number_of_feats = input_2d.size(1)\n",
    "\n",
    "        # the tensor is sorted in desending order along the second dimension the cumulative sum is calculated and \n",
    "        # -1 is subtracted from the cumsum and this will be used as threshold later\n",
    "        input_sorted, _ = torch.sort(input_2d, descending=True, dim=1)\n",
    "        input_cumsum = input_sorted.cumsum(dim=1) - 1\n",
    "        \n",
    "        # a range of values from 1 to `number_of_feats + 1` is created and then a count of how many elements in each\n",
    "        # vector of `input_sorted` are greater than the corresponding element in `input_cumsum / arrange` is calculated\n",
    "        arange = torch.arange(1, number_of_feats + 1, device=input.device)\n",
    "        counts = (input_sorted > input_cumsum / arange).sum(dim=1).unsqueeze(1)\n",
    "        \n",
    "        # the `gather` function is used to select the elements in each row of `input_cumsum` specified by `counts - 1`,\n",
    "        # which gives the cumsum value at the threshold index for each row, and then that is divided by `counts` to get \n",
    "        # threshold for each vector in the input\n",
    "        threshold = input_cumsum.gather(1, counts - 1) / counts.to(input.dtype)\n",
    "\n",
    "        # the threshold is subtracted from the original 2D tensor, and any negative values are clamped to zero. This\n",
    "        # creates sparsity in the output. The tensor is then reshaped back to the original size and the tranpose is undone\n",
    "        output = torch.clamp(input_2d - threshold, min=0)\n",
    "        output = output.view(*original_size).transpose(dim, -1).contiguous()\n",
    "\n",
    "        self.output = output\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        grad_output = grad_output - (grad_output.sum(dim=-1, keepdim=True) * self.output)\n",
    "        return grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, vbs=128, momentum=0.02):\n",
    "        super(AttentionTransformer, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim, bias=False)\n",
    "        self.bn = GBN(output_dim, vbs=vbs, momentum=momentum)\n",
    "        self.sparsemax = Sparsemax()\n",
    "    \n",
    "    def forward(self, priors, processed_feat):\n",
    "        x = self.bn(self.fc(processed_feat))\n",
    "        x = torch.mul(x, priors)\n",
    "        x = self.sparsemax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class FeatureTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, n_shared=2, n_individual=2, vbs=128, momentum=0.02):\n",
    "        super(FeatureTransformer, self).__init__()\n",
    "\n",
    "        # the shared layers are used by all decision steps\n",
    "        self.shared_layers = nn.ModuleList()\n",
    "        for _ in range(n_shared):\n",
    "            self.shared_layers.append(nn.Linear(input_dim, output_dim, bias=False))\n",
    "            self.shared_layers.append(GBN(output_dim, vbs=vbs, momentum=momentum))\n",
    "\n",
    "        self.individual_layers = nn.ModuleList()\n",
    "        for _ in range(n_individual):\n",
    "            self.individual_layers.append(nn.Linear(output_dim, output_dim, bias=False))\n",
    "            self.individual_layers.append(GBN(output_dim, vbs=vbs, momentum=momentum))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.shared_layers:\n",
    "            x = F.relu(layer(x))\n",
    "\n",
    "        for layer in self.individual_layers:\n",
    "            x = F.relu(layer(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, n_steps, vbs=128, momentum=0.02):\n",
    "        super(TabNet, self).__init__()\n",
    "        self.n_steps = n_steps\n",
    "        self.attention_transformers = nn.ModuleList()\n",
    "        self.feature_transformers = nn.ModuleList()\n",
    "        for _ in range(n_steps):\n",
    "            attention_transformer = AttentionTransformer(input_dim, output_dim, vbs=vbs, momentum=momentum)\n",
    "            self.attention_transformers.append(attention_transformer)\n",
    "            feature_transformer = FeatureTransformer(input_dim, output_dim, vbs=vbs, momentum=momentum)\n",
    "            self.feature_transformers.append(feature_transformer)\n",
    "        self.final_mapping = nn.Linear(output_dim, 2, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        prior = torch.ones(x.shape, device=x.device) / x.shape[1]\n",
    "        for step in range(self.n_steps):\n",
    "            attention_score = self.attention_transformers[step](prior, x)\n",
    "            x = self.feature_transformers[step](x)\n",
    "            prior = attention_score\n",
    "        return self.final_mapping(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 80, Loss: 0.738968551158905, Accuracy: 0.5004500150680542\n",
      "Epoch 11 / 80, Loss: 0.7000560164451599, Accuracy: 0.5055500268936157\n",
      "Epoch 21 / 80, Loss: 0.6929605007171631, Accuracy: 0.5163000226020813\n",
      "Epoch 31 / 80, Loss: 0.6907472014427185, Accuracy: 0.5196999907493591\n",
      "Epoch 41 / 80, Loss: 0.6895896196365356, Accuracy: 0.5206999778747559\n",
      "Epoch 51 / 80, Loss: 0.6888134479522705, Accuracy: 0.5232999920845032\n",
      "Epoch 61 / 80, Loss: 0.6895623803138733, Accuracy: 0.5160999894142151\n",
      "Epoch 71 / 80, Loss: 0.689385175704956, Accuracy: 0.5164250135421753\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create a binary classification dataset\n",
    "X, y = make_classification(n_samples=200000, n_features=128, n_informative=5, random_state=42)\n",
    "\n",
    "# split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# scale the features\n",
    "scalar = StandardScaler()\n",
    "X_train = scalar.fit_transform(X_train)\n",
    "X_test = scalar.transform(X_test)\n",
    "\n",
    "# Convert the data into pytorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# define the model\n",
    "input_dim = 128\n",
    "output_dim = 128\n",
    "n_steps = 3\n",
    "vbs = 8\n",
    "momentum = 0.01\n",
    "n_epochs = 80\n",
    "\n",
    "model = TabNet(input_dim, output_dim, n_steps, vbs, momentum)\n",
    "criterion = nn.CrossEntropyLoss()  # for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # train the model\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        # evaluate the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X_test_tensor)\n",
    "            y_pred = y_pred.argmax(dim=1)  # get the class with the highest score\n",
    "            # calcuate accuracy on the test set\n",
    "            accuracy = (y_pred == y_test_tensor).float().mean()\n",
    "\n",
    "            print(f'Epoch {epoch + 1} / {n_epochs}, Loss: {loss.item()}, Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
